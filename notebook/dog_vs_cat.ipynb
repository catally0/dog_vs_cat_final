{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检测python版本\n",
    "这里我们使用的python的版本为3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "为了更好的训练模型，这里对数据进行一定程度对预处理\n",
    "- 删除训练数据集中过大（`500*500`以上）和过小（`100*100`以下）的图片。\n",
    "- 将图片按照一定的目录结构归类。\n",
    "- 删除错误标记的图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from PIL import Image\n",
    "\n",
    "# 删除被错误标记的图片\n",
    "def pick_err_flag(path, bad_path):\n",
    "    bad_list = ['cat.92.jpg',    'cat.724.jpg',   'cat.1450.jpg',  'cat.3216.jpg', 'cat.3822.jpg', 'cat.5351.jpg',\n",
    "                'cat.5418.jpg',  'cat.7377.jpg',  'cat.7564.jpg',  'cat.8456.jpg', 'cat.9171.jpg', 'cat.10029.jpg',\n",
    "                'cat.10712.jpg', 'cat.11184.jpg', 'dog.1259.jpg',  'dog.1835.jpg', 'dog.2614.jpg', 'dog.3889.jpg',\n",
    "                'dog.4367.jpg',  'dog.5604.jpg',  'dog.8736.jpg',  'dog.8898.jpg', 'dog.9517.jpg', 'dog.10161.jpg',\n",
    "                'dog.10190.jpg', 'dog.10237.jpg', 'dog.10401.jpg', 'dog.10797.jpg','dog.10801.jpg','dog.11186.jpg',\n",
    "                'dog.11299.jpg', 'dog.12376.jpg', 'dog.10747.jpg']\n",
    "    for img_name in bad_list:\n",
    "        im_path = os.path.join(path, img_name)\n",
    "        if os.path.exists(im_path):\n",
    "            shutil.move(im_path, os.path.join(bad_path, img_name))\n",
    "    print(len(bad_list))\n",
    "\n",
    "pick_err_flag(\"data/train\", \"data/train_bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 删除不合尺寸\n",
    "def pick_bad_pics(path, bad_path):\n",
    "    bad_list = []\n",
    "    img_list = os.listdir(path)\n",
    "    for img_name in img_list:\n",
    "        im_path = os.path.join(path, img_name)\n",
    "        im = Image.open(im_path)\n",
    "        w, h = im.size\n",
    "        if w > 500 or h > 500 or w < 10 or h < 10:\n",
    "            bad_list.append(img_name)\n",
    "            shutil.move(im_path, os.path.join(bad_path, img_name))\n",
    "    print(len(bad_list))\n",
    "\n",
    "pick_bad_pics(\"data/train\", \"data/train_bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取加载数据集，归一化处理\n",
    "将用于训练的数据集加载到内存，等待处理。主要是转化为ndarray类型到数据，方便后续到计算和处理.因为选择到预训练模型，对于图片到要求都是`299*299`大小，这里我们读取数据时，图片统一调整到这个尺寸。\n",
    "- 加载训练集数据。\n",
    "- 加载测试集数据。\n",
    "- 输出一个经过正规化的、Numpy array 格式的图像数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加载训练集\n",
    "def load_train_data():\n",
    "\n",
    "    cat = glob.glob(\"data/train/cat.*.jpg\")\n",
    "    dog = glob.glob(\"data/train/dog.*.jpg\")\n",
    "    train_data = np.zeros(((len(cat)+len(dog)), 299, 299, 3), dtype=np.uint8)\n",
    "    train_targ = np.array([0]*len(cat) + [1]*len(dog))\n",
    "\n",
    "    i = 0\n",
    "    for img_name in tqdm(cat):\n",
    "        img = cv2.imread(img_name)\n",
    "        train_data[i] = cv2.resize(img,(299, 299))\n",
    "        i += 1\n",
    "    for img_name in tqdm(dog):\n",
    "        img = cv2.imread(img_name)\n",
    "        train_data[i] = cv2.resize(img,(299, 299))\n",
    "        i += 1\n",
    "\n",
    "    return train_data, train_targ\n",
    "\n",
    "# 加载测试集\n",
    "def load_test_data():\n",
    "\n",
    "    test = glob.glob(\"data/test/*.jpg\")\n",
    "    test_data = np.zeros((len(test), 299, 299, 3), dtype=np.uint8)\n",
    "    \n",
    "    i = 0\n",
    "    for img_name in tqdm(test):\n",
    "        img = cv2.imread(img_name)\n",
    "        test_data[i] = cv2.resize(img,(299, 299))\n",
    "        i += 1\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12485/12485 [00:39<00:00, 319.94it/s]\n",
      "100%|██████████| 12480/12480 [00:40<00:00, 310.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# 处理，加载训练集数据\n",
    "train_data, train_targ = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:39<00:00, 314.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# 处理，加载测试集数据\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24965, 299, 299, 3) (24965,) (12500, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, train_targ.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拆分验证集\n",
    "对标记数据进行处理，拆分验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 划分数据\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_data, train_targ, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型InceptionV3\n",
    "预训练模型InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 14s 0us/step\n",
      "InceptionV3 has 315 layers.\n"
     ]
    }
   ],
   "source": [
    "# 构建InceptionV3\n",
    "def buid_inceptionv3():\n",
    "\n",
    "    # 获取基础模型，不保留顶层的全连接网络\n",
    "    input_tensor = keras.Input(shape=(299, 299, 3)) \n",
    "    input_tensor = keras.layers.Lambda(keras.applications.inception_v3.preprocess_input)(input_tensor)\n",
    "    base_model   = keras.applications.inception_v3.InceptionV3(input_tensor=input_tensor, include_top=False)\n",
    "\n",
    "    # 锁定模型，保护处理\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 空域信号施加全局平均池化，dropout处理防止过拟合，重建全连接层\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # 配置模型\n",
    "    result = keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "    result.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 返回\n",
    "    print('InceptionV3 has %d layers.' % len(result.layers))\n",
    "    return result\n",
    "\n",
    "# 创建\n",
    "inceptionv3_obj = buid_inceptionv3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型\n",
    "#keras.utils.plot_model(inceptionv3_obj, to_file='model_inceptionv3.png')\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(inceptionv3_obj).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19813 samples, validate on 4954 samples\n",
      "Epoch 1/10\n",
      "19813/19813 [==============================] - 286s 14ms/step - loss: 0.2033 - acc: 0.9290 - val_loss: 0.0484 - val_acc: 0.9903\n",
      "Epoch 2/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.1057 - acc: 0.9628 - val_loss: 0.0658 - val_acc: 0.9794\n",
      "Epoch 3/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0910 - acc: 0.9664 - val_loss: 0.1326 - val_acc: 0.9532\n",
      "Epoch 4/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0876 - acc: 0.9680 - val_loss: 0.0704 - val_acc: 0.9782\n",
      "Epoch 5/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0853 - acc: 0.9669 - val_loss: 0.0593 - val_acc: 0.9820\n",
      "Epoch 6/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0852 - acc: 0.9684 - val_loss: 0.0975 - val_acc: 0.9671\n",
      "Epoch 7/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0822 - acc: 0.9686 - val_loss: 0.1360 - val_acc: 0.9556\n",
      "Epoch 8/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0810 - acc: 0.9686 - val_loss: 0.0788 - val_acc: 0.9766\n",
      "Epoch 9/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0804 - acc: 0.9694 - val_loss: 0.0502 - val_acc: 0.9851\n",
      "Epoch 10/10\n",
      "19813/19813 [==============================] - 277s 14ms/step - loss: 0.0801 - acc: 0.9703 - val_loss: 0.0806 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe79dc9a3c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "inceptionv3_obj.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测输出\n",
    "inceptionv3_predict = inceptionv3_obj.predict(test_data)\n",
    "inceptionv3_predict = inceptionv3_predict.clip(min=0.005, max=0.995)\n",
    "inceptionv3_predict = inceptionv3_predict.flatten(order = 'F')\n",
    "\n",
    "# 保存结果\n",
    "submission = pd.DataFrame(data = {'id':(np.arange(len(test_data))+1), 'label': inceptionv3_predict})\n",
    "submission.to_csv('inceptionv3_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型Xception\n",
    "预训练模型Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 16s 0us/step\n",
      "xception has 136 layers.\n"
     ]
    }
   ],
   "source": [
    "# 构建xception\n",
    "def buid_xception():\n",
    "\n",
    "    # 获取基础模型，不保留顶层的全连接网络\n",
    "    input_tensor = keras.Input(shape=(299, 299, 3)) \n",
    "    input_tensor = keras.layers.Lambda(keras.applications.xception.preprocess_input)(input_tensor)\n",
    "    base_model   = keras.applications.xception.Xception(input_tensor=input_tensor, include_top=False)\n",
    "\n",
    "    # 锁定模型，保护处理\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 空域信号施加全局平均池化，dropout处理防止过拟合，重建全连接层\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    # 配置模型\n",
    "    result = keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "    result.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 返回\n",
    "    print('xception has %d layers.' % len(result.layers))\n",
    "    return result\n",
    "\n",
    "# 创建模型\n",
    "xception_obj = buid_xception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型\n",
    "#keras.utils.plot_model(xception_obj, to_file='model_xception.png')\n",
    "#SVG(model_to_dot(xception_obj).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19813 samples, validate on 4954 samples\n",
      "Epoch 1/10\n",
      "19813/19813 [==============================] - 498s 25ms/step - loss: 0.1458 - acc: 0.9641 - val_loss: 0.0931 - val_acc: 0.9782\n",
      "Epoch 2/10\n",
      "19813/19813 [==============================] - 493s 25ms/step - loss: 0.0571 - acc: 0.9841 - val_loss: 0.0632 - val_acc: 0.9826\n",
      "Epoch 3/10\n",
      "19813/19813 [==============================] - 493s 25ms/step - loss: 0.0491 - acc: 0.9843 - val_loss: 0.0631 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "19813/19813 [==============================] - 493s 25ms/step - loss: 0.0440 - acc: 0.9858 - val_loss: 0.0569 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "19813/19813 [==============================] - 492s 25ms/step - loss: 0.0417 - acc: 0.9853 - val_loss: 0.0664 - val_acc: 0.9792\n",
      "Epoch 6/10\n",
      "19813/19813 [==============================] - 492s 25ms/step - loss: 0.0405 - acc: 0.9856 - val_loss: 0.0786 - val_acc: 0.9754\n",
      "Epoch 7/10\n",
      "19813/19813 [==============================] - 492s 25ms/step - loss: 0.0394 - acc: 0.9858 - val_loss: 0.0767 - val_acc: 0.9758\n",
      "Epoch 8/10\n",
      "19813/19813 [==============================] - 493s 25ms/step - loss: 0.0384 - acc: 0.9875 - val_loss: 0.0765 - val_acc: 0.9756\n",
      "Epoch 9/10\n",
      "19813/19813 [==============================] - 492s 25ms/step - loss: 0.0382 - acc: 0.9866 - val_loss: 0.0737 - val_acc: 0.9762\n",
      "Epoch 10/10\n",
      "19813/19813 [==============================] - 493s 25ms/step - loss: 0.0357 - acc: 0.9880 - val_loss: 0.0643 - val_acc: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7a050b978>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "xception_obj.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测输出\n",
    "xception_predict = xception_obj.predict(test_data)\n",
    "xception_predict = xception_predict.clip(min=0.005, max=0.995)\n",
    "xception_predict = xception_predict.flatten(order = 'F')\n",
    "\n",
    "# 保存结果\n",
    "submission = pd.DataFrame(data = {'id':(np.arange(len(test_data))+1), 'label': xception_predict})\n",
    "submission.to_csv('xception_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型Inception ResnetV2\n",
    "预训练模型Inception ResnetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 37s 0us/step\n",
      "Inception ResnetV2 has 784 layers.\n"
     ]
    }
   ],
   "source": [
    "# 构建Inception ResnetV2\n",
    "def buid_inception_resnet_v2():\n",
    "\n",
    "    # 获取基础模型，不保留顶层的全连接网络\n",
    "    input_tensor = keras.Input(shape=(299, 299, 3)) \n",
    "    input_tensor = keras.layers.Lambda(keras.applications.inception_resnet_v2.preprocess_input)(input_tensor)\n",
    "    base_model = keras.applications.inception_resnet_v2.InceptionResNetV2(input_tensor=input_tensor, include_top=False)\n",
    "\n",
    "    # 锁定模型，保护处理\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # 空域信号施加全局平均池化，dropout处理防止过拟合，重建全连接层\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    # 配置模型\n",
    "    result = keras.models.Model(inputs=base_model.input, outputs=x)\n",
    "    result.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 返回\n",
    "    print('Inception ResnetV2 has %d layers.' % len(result.layers))\n",
    "    return result\n",
    "\n",
    "# 创建\n",
    "inception_resnet_v2_obj = buid_inception_resnet_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型\n",
    "#keras.utils.plot_model(inception_resnet_v2_obj, to_file='model_inception_resnet_v2.png')\n",
    "#SVG(model_to_dot(inception_resnet_v2_obj).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19813 samples, validate on 4954 samples\n",
      "Epoch 1/10\n",
      "19813/19813 [==============================] - 712s 36ms/step - loss: 0.1755 - acc: 0.9453 - val_loss: 0.1235 - val_acc: 0.9578\n",
      "Epoch 2/10\n",
      "19813/19813 [==============================] - 654s 33ms/step - loss: 0.0862 - acc: 0.9722 - val_loss: 0.0627 - val_acc: 0.9806\n",
      "Epoch 3/10\n",
      "19813/19813 [==============================] - 654s 33ms/step - loss: 0.0749 - acc: 0.9738 - val_loss: 0.0867 - val_acc: 0.9727\n",
      "Epoch 4/10\n",
      "19813/19813 [==============================] - 655s 33ms/step - loss: 0.0685 - acc: 0.9753 - val_loss: 0.0595 - val_acc: 0.9836\n",
      "Epoch 5/10\n",
      "19813/19813 [==============================] - 655s 33ms/step - loss: 0.0635 - acc: 0.9770 - val_loss: 0.0548 - val_acc: 0.9855\n",
      "Epoch 6/10\n",
      "19813/19813 [==============================] - 655s 33ms/step - loss: 0.0607 - acc: 0.9768 - val_loss: 0.0620 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "19813/19813 [==============================] - 656s 33ms/step - loss: 0.0624 - acc: 0.9771 - val_loss: 0.0362 - val_acc: 0.9907\n",
      "Epoch 8/10\n",
      "19813/19813 [==============================] - 652s 33ms/step - loss: 0.0619 - acc: 0.9773 - val_loss: 0.0655 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "19813/19813 [==============================] - 650s 33ms/step - loss: 0.0632 - acc: 0.9774 - val_loss: 0.0644 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "19813/19813 [==============================] - 648s 33ms/step - loss: 0.0625 - acc: 0.9771 - val_loss: 0.0565 - val_acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f675978cc18>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "inception_resnet_v2_obj.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测输出\n",
    "inception_resnet_v2_predict = inception_resnet_v2_obj.predict(test_data)\n",
    "inception_resnet_v2_predict = inception_resnet_v2_predict.clip(min=0.005, max=0.995)\n",
    "inception_resnet_v2_predict = inception_resnet_v2_predict.flatten(order = 'F')\n",
    "\n",
    "# 保存结果\n",
    "submission = pd.DataFrame(data = {'id':(np.arange(len(test_data))+1), 'label': inception_resnet_v2_predict})\n",
    "submission.to_csv('inception_resnet_v2_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取特征，融合模型\n",
    "将多个模型到特征向量融合训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# 提取模型的特征数据\n",
    "def pick_features(raw_model, pre_input):\n",
    "\n",
    "    # 获取基础模型，不保留顶层的全连接网络\n",
    "    inputs       = keras.Input(shape=(299, 299, 3))\n",
    "    input_tensor = keras.layers.Lambda(pre_input)(inputs)\n",
    "    base_model   = raw_model(input_tensor=input_tensor, include_top=False)\n",
    "\n",
    "    # 提取特征数据\n",
    "    x     = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    train_feature = model.predict(x_train, batch_size=64)\n",
    "    test_feature  = model.predict(test_data, batch_size=64)\n",
    "\n",
    "    # 返回\n",
    "    with h5py.File(\"feature_%s.h5\" % raw_model.__name__, 'w') as f:\n",
    "        f.create_dataset('train', data=train_feature)\n",
    "        f.create_dataset('test',  data=test_feature)\n",
    "        f.create_dataset('label', data=y_train)\n",
    "    #return train_feature, test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取inceptionv3特征数据\n",
    "pick_features(keras.applications.inception_v3.InceptionV3, keras.applications.inception_v3.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取xception特征数据\n",
    "pick_features(keras.applications.xception.Xception, keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取inception_resnet_v2特征数据\n",
    "pick_features(keras.applications.inception_resnet_v2.InceptionResNetV2, keras.applications.inception_resnet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "构建最终的模型，用特征数据进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= [], []\n",
    "for fetchfile in ['feature_Xception.h5', 'feature_InceptionV3.h5', 'feature_InceptionResNetV2.h5']:\n",
    "    with h5py.File(fetchfile, 'r') as h:\n",
    "        train.append(np.array(h['train']))\n",
    "        test.append(np.array(h['test']))\n",
    "        label = np.array(h['label'])\n",
    "        \n",
    "train_data_feature = np.concatenate(train, axis=1)\n",
    "test_data_feature  = np.concatenate(test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ft_train, x_ft_valid, y_ft_train, y_ft_valid = train_test_split(train_data_feature, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建融合模型\n",
    "def buid_final_model():\n",
    "\n",
    "    inputs = keras.Input(shape=(x_ft_train.shape[1],))\n",
    "    x = keras.layers.Dropout(0.4)(inputs)\n",
    "    x = keras.layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal')(x)\n",
    "\n",
    "    # 配置模型\n",
    "    final_model = keras.models.Model(inputs=inputs, outputs=x)\n",
    "    final_model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 返回\n",
    "    return final_model\n",
    "\n",
    "\n",
    "# 最终模型\n",
    "final_model_obj = buid_final_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化展示\n",
    "展示构建的最终模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化模型\n",
    "#keras.utils.plot_model(final_model_obj, to_file='model.png')\n",
    "\n",
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "#SVG(model_to_dot(final_model_obj).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15977 samples, validate on 3995 samples\n",
      "Epoch 1/10\n",
      "15977/15977 [==============================] - 20s 1ms/step - loss: 0.0466 - acc: 0.9886 - val_loss: 0.0179 - val_acc: 0.9955\n",
      "Epoch 2/10\n",
      "15977/15977 [==============================] - 2s 116us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0133 - val_acc: 0.9942\n",
      "Epoch 3/10\n",
      "15977/15977 [==============================] - 2s 115us/step - loss: 0.0144 - acc: 0.9957 - val_loss: 0.0129 - val_acc: 0.9952\n",
      "Epoch 4/10\n",
      "15977/15977 [==============================] - 2s 117us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0136 - val_acc: 0.9945\n",
      "Epoch 5/10\n",
      "15977/15977 [==============================] - 2s 118us/step - loss: 0.0123 - acc: 0.9971 - val_loss: 0.0126 - val_acc: 0.9952\n",
      "Epoch 6/10\n",
      "15977/15977 [==============================] - 2s 115us/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.0140 - val_acc: 0.9947\n",
      "Epoch 7/10\n",
      "15977/15977 [==============================] - 2s 117us/step - loss: 0.0108 - acc: 0.9975 - val_loss: 0.0127 - val_acc: 0.9955\n",
      "Epoch 8/10\n",
      "15977/15977 [==============================] - 2s 127us/step - loss: 0.0098 - acc: 0.9976 - val_loss: 0.0135 - val_acc: 0.9957\n",
      "Epoch 9/10\n",
      "15977/15977 [==============================] - 2s 120us/step - loss: 0.0092 - acc: 0.9974 - val_loss: 0.0125 - val_acc: 0.9945\n",
      "Epoch 10/10\n",
      "15977/15977 [==============================] - 2s 116us/step - loss: 0.0095 - acc: 0.9975 - val_loss: 0.0143 - val_acc: 0.9960\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 训练最终模型\n",
    "def train_final_model(final_model):\n",
    "\n",
    "    # 模型保存位置\n",
    "    logs_file = 'ft_extract_features-{val_loss:.4f}.h5'\n",
    "    path = os.getcwd()\n",
    "    path_logs = os.path.join(path, logs_file)\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    model_check = keras.callbacks.ModelCheckpoint(path_logs, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    final_model.fit(x_ft_train, y_ft_train, batch_size=64, epochs=10, \n",
    "               validation_data=(x_ft_valid, y_ft_valid), callbacks=[early_stop, model_check])\n",
    "\n",
    "# 训练\n",
    "train_final_model(final_model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测输出\n",
    "final_predict = final_model_obj.predict(test_data_feature)\n",
    "final_predict = final_predict.clip(min=0.005, max=0.995)\n",
    "final_predict = final_predict.flatten(order = 'F')\n",
    "\n",
    "# 保存结果\n",
    "submission = pd.DataFrame(data = {'id':(np.arange(len(test_data))+1), 'label': final_predict})\n",
    "submission.to_csv('final_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数损失曲线\n",
    "绘制模型的对数损失曲线，直观展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 对数损失更新曲线\n",
    "def show_loss(final_model):\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    his_model = final_model.history\n",
    "    history = his_model.history\n",
    "    ax[0].plot(history['loss'], color='b', label=\"loss\")\n",
    "    ax[0].plot(history['val_loss'], color='r', label=\"val_loss\",axes =ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history['acc'], color='g', label=\"acc\")\n",
    "    ax[1].plot(history['val_acc'], color='c',label=\"val_acc\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "\n",
    "# 绘制\n",
    "show_loss(final_model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
